# -*- coding: utf-8 -*-
"""Copy of Resnet-ImageNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k6Azry5WeA6dAreNF8ES4E59VwsxoX3O
"""

# from google.colab import drive
# drive.mount('/content/drive')

# ! unzip '/content/drive/My Drive/tiny-imagenet-200.zip'

# %cd /content/drive/My\ Drive/Assignment\ 4/

import os
import cv2
import imutils
import json
import imutils
import numpy as np
from dataset_utils import HDF5DatasetWriter
from sklearn.preprocessing import LabelEncoder
from imutils import paths

TRAIN = "../tiny-imagenet-200/train/"
VAL = "../tiny-imagenet-200/val/images"
VAL_ANNOT = "../tiny-imagenet-200/val/val_annotations.txt"
WORDNET = "../tiny-imagenet-200/wnits.txt"
WORD_LABELS = "../tiny-imagenet-200/words.txt"

CLASSES = 200
# UM_IMAGES = 500 * CLASSES

TRAIN_HF5 = "../tiny-imagenet-200/hf5/train.hdf5"
VAL_HF5 = "../tiny-imagenet-200/hf5/val.hdf5"

MEAN_NORM = "tiny-image-net-200-mean.json"
OUTPUT = "output"

le = LabelEncoder() 

# Train set configurations
train_paths = list(paths.list_images(TRAIN))
train_labels = [x.split(os.path.sep)[-3] for x in train_paths]
train_labels = le.fit_transform(train_labels)

# Validation set configurations
temp = open(VAL_ANNOT).read().strip().split("\n")
temp = [x.split("\t")[:2] for x in temp]
val_paths = [os.path.sep.join([VAL, x[0]]) for x in temp]
val_labels = [x[1] for x in temp]
val_labels = le.fit_transform(val_labels)

datasets = [
	("train", train_paths, train_labels, TRAIN_HF5),
	("val", val_paths, val_labels, VAL_HF5)
]

(R, G, B) = ([], [], [])

for (dType, paths, labels, outputPath) in datasets:
	writer = HDF5DatasetWriter((len(paths), 64, 64, 3), outputPath)

	for (i, (path, label)) in enumerate(zip(paths, labels)):
		image = cv2.imread(path)

		# Storing mean averages of train set data
		if dType == "train":
			(b, g, r) = cv2.mean(image)[:3]
			R.append(r)
			G.append(g)
			B.append(b)
		
		# Add the image and label to the HDF5 dataset
		writer.add([image], [label])

	writer.close()

means = {"R": np.mean(R), "G": np.mean(G), "B": np.mean(B)}
file = open(MEAN_NORM, "w")
file.write(json.dumps(means))
file.close()

import preprocess
from preprocess import MeanPreprocessor, ImageToArrayPreprocessor
from dataset_utils import *
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD, Adam
from keras.models import load_model
import keras.backend as K
import json
import sys

class SimplePreprocessor:
	def __init__(self, width, height, inter=cv2.INTER_AREA):
		# store the target image width, height, and interpolation
		# method used when resizing
		self.width = width
		self.height = height
		self.inter = inter

	def preprocess(self, image):
		# resize the image to a fixed size, ignoring the aspect
		# ratio
		return cv2.resize(image, (self.width, self.height),
			interpolation=self.inter)

# Data Augmentation

aug = ImageDataGenerator(
    rotation_range=18, 
    zoom_range=0.15,
    width_shift_range=0.2, 
    height_shift_range=0.2, 
    shear_range=0.15,
    horizontal_flip=True, 
    fill_mode="nearest")

means = json.loads(open(MEAN_NORM).read())

# Preprocessing

sp = SimplePreprocessor(64, 64)
mp = preprocess.MeanPreprocessor(means["R"], means["G"], means["B"])
iap = ImageToArrayPreprocessor()

train_gen = HDF5DatasetGenerator(TRAIN_HF5, 64, aug=aug,
                                preprocessors=[sp, mp, iap], classes=200)
val_gen = HDF5DatasetGenerator(VAL_HF5, 64,
                              preprocessors=[sp, mp, iap], classes=200)

from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D, SeparableConv2D
from keras.layers.convolutional import AveragePooling2D, MaxPooling2D, ZeroPadding2D
from keras.layers.core import Activation, Dense
from CyclicLearningRate.clr_callback import *
from keras.layers import Flatten, Input, add
from keras.optimizers import Adam
from keras.callbacks import *
from keras.models import Model
from keras.regularizers import l2
from keras import backend as K

class ResNet:
	@staticmethod
	def residual_module(data, K, stride, chanDim, red=False, reg=0.0001, bnEps=2e-5, bnMom=0.9):
		shortcut = data

		bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(data)
		act1 = Activation("relu")(bn1)
		conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act1)

		bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(conv1)
		act2 = Activation("relu")(bn2)
		conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride, padding="same", use_bias=False, kernel_regularizer=l2(reg))(act2)

		bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(conv2)
		act3 = Activation("relu")(bn3)
		conv3 = Conv2D(K, (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act3)

		if red:
			shortcut = Conv2D(K, (1, 1), strides=stride, use_bias=False, kernel_regularizer=l2(reg))(act1)

		x = add([conv3, shortcut])

		return x

	@staticmethod
	def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9):
		inputShape = (height, width, depth)
		chanDim = -1

		inputs = Input(shape=inputShape)
		x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(inputs)

		x = Conv2D(filters[0], (5, 5), use_bias=False, padding="same", kernel_regularizer=l2(reg))(x)
		x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(x)
		x = Activation("relu")(x)
		x = ZeroPadding2D((1, 1))(x)
		x = MaxPooling2D((3, 3), strides=(2, 2))(x)
    
		for i in range(0, len(stages)):
			stride = (1, 1) if i == 0 else (2, 2)
			x = ResNet.residual_module(x, filters[i + 1], stride, chanDim, red=True, bnEps=bnEps, bnMom=bnMom)

			for j in range(0, stages[i] - 1):
				x = ResNet.residual_module(x, filters[i + 1], (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)

		x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(x)
		x = Activation("relu")(x)
		x = AveragePooling2D((8, 8))(x)
  
		x = Conv2D(200, (1,1), kernel_regularizer=l2(reg))(x)
		x = Flatten()(x)
		x = Activation("softmax")(x)

		model = Model(inputs, x, name="resnet")

		return model

model = ResNet.build(64, 64, 3, 200, (3, 4, 6), (64, 128, 256, 512), reg=0.0005)

from keras.callbacks import Callback

class EpochCheckpoint(Callback):
	def __init__(self, outputPath, every=5, startAt=0):
		# call the parent constructor
		super(Callback, self).__init__()
    
		self.outputPath = outputPath
		self.every = every
		self.intEpoch = startAt

	def on_epoch_end(self, epoch, logs={}):
		# check to see if the model should be serialized to disk
		if (self.intEpoch + 1) % self.every == 0:
			p = os.path.sep.join([self.outputPath,
				"resnet.hdf5".format(self.intEpoch + 1)])
			self.model.save(p, overwrite=True)
		self.intEpoch += 1

# Learning Rate Decay

from keras.callbacks import LearningRateScheduler

NUM_EPOCHS = 75
INIT_LR = le-1

def poly_decay(epoch):
  maxEpochs = NUM_EPOCHS
  baseLR = INIT_LR
  power = 1.0
  
  # compute the new learning rate based on polynomial decay
  alpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power
  
  # return the new learning rate
  return alpha

import tensorflow
flag = 0

# checkpoint_path = "../checkpoints/check.ckpt"
# checkpoint_dir = os.path.dirname(checkpoint_path)

# Create checkpoint callback

# cp_callback = tensorflow.keras.callbacks.ModelCheckpoint(checkpoint_path, 
                                                 # save_weights_only=False,
                                                 # verbose=1, period=5)

# callbacks = [
#   EpochCheckpoint('../checkpoints/', every=5),
#     cp_callback,
#     LearningRateScheduler(poly_decay)
# ]

from keras.models import load_model
if flag == 0: 
  model = ResNet.build(64, 64, 3, 200, (3, 4, 6), (64, 128, 256, 512), reg=0.0005)
  opt = SGD(lr=INIT_LR, momentum=0.9)
  model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])
#   model.compile(optimizer=Adam(0.1), loss="categorical_crossentropy", metrics=["accuracy"])

else:
  
  # Load the model
  model = load_model('../checkpoints/resnet.hdf5')
  
# model.compile(optimizer=Adam(0.1), loss="categorical_crossentropy", metrics=["accuracy"])
#   Update the learning rate
#   print(f'Old Learning Rate: {K.get_value(model.optimizer.lr)}')
#   K.set_value(model.optimizer.lr, le-5)
#   print(f'New Learning Rate: {K.get_value(model.optimizer.lr)}')

model.summary()

model.fit_generator(
  train_gen.generator(),
  steps_per_epoch=train_gen.numImages // 64,
  validation_data=val_gen.generator(),
  validation_steps=val_gen.numImages // 64,
  epochs=50,
  max_queue_size=128,
  verbose=1
)

model.save('final.hdf5')

# close the databases
train_gen.close()
val_gen.close()

